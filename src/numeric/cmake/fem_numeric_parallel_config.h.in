#pragma once
/**
 * @file fem_numeric_parallel_config.h
 * @brief Auto-generated configuration header for FEM Numeric parallel support
 *
 * This file is automatically generated by CMake based on available
 * parallelism libraries and configuration options.
 */

#cmakedefine FEM_NUMERIC_PARALLEL_ENABLED
#cmakedefine FEM_NUMERIC_HAS_OPENMP
#cmakedefine FEM_NUMERIC_HAS_TBB
#cmakedefine FEM_NUMERIC_HAS_MPI
#cmakedefine FEM_NUMERIC_MPI_OPTIONAL
#cmakedefine FEM_NUMERIC_USE_TBB_MALLOC

// Hybrid configurations
#cmakedefine FEM_NUMERIC_HYBRID_OPENMP_TBB
#cmakedefine FEM_NUMERIC_HYBRID_MPI_OPENMP
#cmakedefine FEM_NUMERIC_HYBRID_MPI_TBB
#cmakedefine FEM_NUMERIC_HYBRID_FULL

// Include appropriate headers
#ifdef FEM_NUMERIC_HAS_OPENMP
#include <omp.h>
#endif

#ifdef FEM_NUMERIC_HAS_TBB
#include <tbb/tbb.h>
#include <tbb/parallel_for.h>
#include <tbb/parallel_reduce.h>
#include <tbb/blocked_range.h>
#endif

#ifdef FEM_NUMERIC_HAS_MPI
#include <mpi.h>
#endif

#include <cstddef>
#include <algorithm>
#include <functional>

namespace fem::numeric::parallel {

// ============================================================================
// Runtime Detection Functions
// ============================================================================

inline constexpr bool openmp_available() {
#ifdef FEM_NUMERIC_HAS_OPENMP
    return true;
#else
    return false;
#endif
}

inline constexpr bool tbb_available() {
#ifdef FEM_NUMERIC_HAS_TBB
    return true;
#else
    return false;
#endif
}

inline constexpr bool mpi_available() {
#ifdef FEM_NUMERIC_HAS_MPI
    return true;
#else
    return false;
#endif
}

inline constexpr bool any_available() {
    return openmp_available() || tbb_available() || mpi_available();
}

inline constexpr bool hybrid_available() {
#if defined(FEM_NUMERIC_HYBRID_OPENMP_TBB) || \
    defined(FEM_NUMERIC_HYBRID_MPI_OPENMP) || \
    defined(FEM_NUMERIC_HYBRID_MPI_TBB) || \
    defined(FEM_NUMERIC_HYBRID_FULL)
    return true;
#else
    return false;
#endif
}

// ============================================================================
// Thread/Process Information
// ============================================================================

inline int num_threads() {
    int threads = 1;

#ifdef FEM_NUMERIC_HAS_OPENMP
    threads = std::max(threads, omp_get_max_threads());
#endif

#ifdef FEM_NUMERIC_HAS_TBB
    threads = std::max(threads, static_cast<int>(tbb::task_arena::max_concurrency()));
#endif

    return threads;
}

inline int mpi_rank() {
#ifdef FEM_NUMERIC_HAS_MPI
    int rank = 0;
    int initialized = 0;
    MPI_Initialized(&initialized);
    if (initialized) {
        MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    }
    return rank;
#else
    return 0;
#endif
}

inline int mpi_size() {
#ifdef FEM_NUMERIC_HAS_MPI
    int size = 1;
    int initialized = 0;
    MPI_Initialized(&initialized);
    if (initialized) {
        MPI_Comm_size(MPI_COMM_WORLD, &size);
    }
    return size;
#else
    return 1;
#endif
}

// ============================================================================
// Parallel Execution Policies
// ============================================================================

enum class ExecutionPolicy {
    Sequential,
    OpenMP,
    TBB,
    Hybrid,
    Auto
};

inline ExecutionPolicy default_policy() {
#ifdef FEM_NUMERIC_HYBRID_FULL
    return ExecutionPolicy::Hybrid;
#elif defined(FEM_NUMERIC_HAS_TBB)
    return ExecutionPolicy::TBB;
#elif defined(FEM_NUMERIC_HAS_OPENMP)
    return ExecutionPolicy::OpenMP;
#else
    return ExecutionPolicy::Sequential;
#endif
}

// ============================================================================
// Unified Parallel For Loop
// ============================================================================

template<typename IndexType, typename Func>
inline void parallel_for(IndexType begin, IndexType end, Func&& func,
                        ExecutionPolicy policy = ExecutionPolicy::Auto) {

    if (policy == ExecutionPolicy::Auto) {
        policy = default_policy();
    }

    switch (policy) {
        case ExecutionPolicy::Sequential:
            for (IndexType i = begin; i < end; ++i) {
                func(i);
            }
            break;

#ifdef FEM_NUMERIC_HAS_OPENMP
        case ExecutionPolicy::OpenMP:
            #pragma omp parallel for
            for (IndexType i = begin; i < end; ++i) {
                func(i);
            }
            break;
#endif

#ifdef FEM_NUMERIC_HAS_TBB
        case ExecutionPolicy::TBB:
            tbb::parallel_for(tbb::blocked_range<IndexType>(begin, end),
                [&func](const tbb::blocked_range<IndexType>& range) {
                    for (IndexType i = range.begin(); i != range.end(); ++i) {
                        func(i);
                    }
                });
            break;
#endif

        case ExecutionPolicy::Hybrid:
#if defined(FEM_NUMERIC_HYBRID_OPENMP_TBB)
            // Use TBB for task scheduling, OpenMP for inner loops
            tbb::parallel_for(tbb::blocked_range<IndexType>(begin, end),
                [&func](const tbb::blocked_range<IndexType>& range) {
                    #pragma omp parallel for
                    for (IndexType i = range.begin(); i < range.end(); ++i) {
                        func(i);
                    }
                });
#elif defined(FEM_NUMERIC_HAS_OPENMP)
            #pragma omp parallel for
            for (IndexType i = begin; i < end; ++i) {
                func(i);
            }
#else
            for (IndexType i = begin; i < end; ++i) {
                func(i);
            }
#endif
            break;

        default:
            // Fallback to sequential
            for (IndexType i = begin; i < end; ++i) {
                func(i);
            }
            break;
    }
}

// ============================================================================
// Unified Parallel Reduction
// ============================================================================

template<typename IndexType, typename T, typename BinaryOp, typename UnaryOp>
inline T parallel_reduce(IndexType begin, IndexType end, T init,
                        BinaryOp&& binary_op, UnaryOp&& unary_op,
                        ExecutionPolicy policy = ExecutionPolicy::Auto) {

    if (policy == ExecutionPolicy::Auto) {
        policy = default_policy();
    }

    switch (policy) {
        case ExecutionPolicy::Sequential: {
            T result = init;
            for (IndexType i = begin; i < end; ++i) {
                result = binary_op(result, unary_op(i));
            }
            return result;
        }

#ifdef FEM_NUMERIC_HAS_OPENMP
        case ExecutionPolicy::OpenMP: {
            T result = init;
            #pragma omp parallel for reduction(+:result)
            for (IndexType i = begin; i < end; ++i) {
                result = binary_op(result, unary_op(i));
            }
            return result;
        }
#endif

#ifdef FEM_NUMERIC_HAS_TBB
        case ExecutionPolicy::TBB: {
            return tbb::parallel_reduce(
                tbb::blocked_range<IndexType>(begin, end),
                init,
                [&](const tbb::blocked_range<IndexType>& range, T local_init) {
                    T result = local_init;
                    for (IndexType i = range.begin(); i != range.end(); ++i) {
                        result = binary_op(result, unary_op(i));
                    }
                    return result;
                },
                binary_op
            );
        }
#endif

        default: {
            // Fallback to sequential
            T result = init;
            for (IndexType i = begin; i < end; ++i) {
                result = binary_op(result, unary_op(i));
            }
            return result;
        }
    }
}

// ============================================================================
// Thread Pool Management
// ============================================================================

class ThreadPool {
public:
    static ThreadPool& instance() {
        static ThreadPool pool;
        return pool;
    }

    void configure(int num_threads = -1) {
        if (num_threads <= 0) {
            num_threads = parallel::num_threads();
        }

#ifdef FEM_NUMERIC_HAS_OPENMP
        omp_set_num_threads(num_threads);
#endif

#ifdef FEM_NUMERIC_HAS_TBB
        if (tbb_arena_) {
            tbb_arena_.reset();
        }
        tbb_arena_ = std::make_unique<tbb::task_arena>(num_threads);
#endif
    }

    template<typename Func>
    void execute(Func&& func) {
#ifdef FEM_NUMERIC_HAS_TBB
        if (tbb_arena_) {
            tbb_arena_->execute(std::forward<Func>(func));
        } else {
            func();
        }
#else
        func();
#endif
    }

private:
    ThreadPool() = default;

#ifdef FEM_NUMERIC_HAS_TBB
    std::unique_ptr<tbb::task_arena> tbb_arena_;
#endif
};

} // namespace fem::numeric::parallel